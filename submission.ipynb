{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Intelligent Systems 3: Probabilistic and Deep Learning\n",
    "#### Exam number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import lstsq\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io,transform\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, ToPILImage, RandomCrop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Using Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data for the duration of this question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "outputs": [],
   "source": [
    "target = df[\"D\"].copy()\n",
    "df[\"D\"] = target.shift(1,fill_value=target[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp/ipykernel_69068/213527618.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{columns}_Lag_{i}\"] = df[columns].shift(i, fill_value=fill_value)\n"
     ]
    }
   ],
   "source": [
    "for index, columns in enumerate([\"A\", \"B\", \"C\", \"D\"]):\n",
    "    for i in range(len(df)):\n",
    "        fill_value = df[columns].iloc[0]\n",
    "        df[f\"{columns}_Lag_{i}\"] = df[columns].shift(i, fill_value=fill_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "outputs": [],
   "source": [
    "df.drop([\"A\",\"B\",\"C\",\"D\"],axis=1,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "outputs": [],
   "source": [
    "scaled= scaler.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data is split into a 2 sets, the training set and the testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df,target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression()"
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8687553221978161"
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, model.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regularisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "outputs": [],
   "source": [
    "myalphas = np.logspace(-6, 6, 13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ridge Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [],
   "source": [
    "model = RidgeCV()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [],
   "source": [
    "ridge_reg = model.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9361216524510323"
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.score(x_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lasso Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "outputs": [],
   "source": [
    "model = LassoCV(alphas=myalphas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.852e-01, tolerance: 1.381e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e-01, tolerance: 1.381e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.001e-02, tolerance: 1.381e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e-01, tolerance: 1.358e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.245e-01, tolerance: 1.358e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e-02, tolerance: 1.358e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.896e-01, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e-01, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.731e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.769e-01, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.153e-01, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.700e-02, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e-01, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.302e-01, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.262e-02, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.255e-01, tolerance: 1.701e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = model.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [
    {
     "data": {
      "text/plain": "0.930240379347723"
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_reg.score(x_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Elastic Net Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [],
   "source": [
    "model = ElasticNetCV()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e-02, tolerance: 1.381e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.670e-02, tolerance: 1.381e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.689e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.942e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.311e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.981e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.632e-02, tolerance: 1.403e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e-02, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e-02, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e-02, tolerance: 1.224e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.446e-02, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.707e-02, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.687e-02, tolerance: 1.434e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "G:\\anaconda\\envs\\workspace\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.279e-01, tolerance: 1.701e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = model.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9279761798747682"
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.score(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Polynomial Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(3),Ridge(alpha=0.1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n                ('ridge', Ridge(alpha=0.1))])"
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train[[\"A_Lag_0\",\"B_Lag_0\",\"C_Lag_0\",\"D_Lag_0\"]], y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8859730500582567"
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test[[\"A_Lag_0\",\"B_Lag_0\",\"C_Lag_0\",\"D_Lag_0\"]],y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Piecewise Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [],
   "source": [
    "ramp = lambda u: np.maximum( u, 0 )\n",
    "step = lambda u: ( u > 0 ).astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [],
   "source": [
    "def SegmentedLinearReg( X, Y, breakpoints ):\n",
    "    nIterationMax = 10\n",
    "\n",
    "    breakpoints = np.sort( np.array(breakpoints) )\n",
    "\n",
    "    dt = np.min( np.diff(X) )\n",
    "    ones = np.ones_like(X)\n",
    "\n",
    "    for i in range( nIterationMax ):\n",
    "        # Linear regression:  solve A*p = Y\n",
    "        Rk = [ramp( X - xk ) for xk in breakpoints ]\n",
    "        Sk = [step( X - xk ) for xk in breakpoints ]\n",
    "        A = np.array([ ones, X ] + Rk + Sk )\n",
    "        p =  lstsq(A.transpose(), Y, rcond=None)[0]\n",
    "\n",
    "        # Parameters identification:\n",
    "        a, b = p[0:2]\n",
    "        ck = p[ 2:2+len(breakpoints) ]\n",
    "        dk = p[ 2+len(breakpoints): ]\n",
    "\n",
    "        # Estimation of the next break-points:\n",
    "        newBreakpoints = breakpoints - dk/ck\n",
    "\n",
    "        # Stop condition\n",
    "        if np.max(np.abs(newBreakpoints - breakpoints)) < dt/5:\n",
    "            break\n",
    "\n",
    "        breakpoints = newBreakpoints\n",
    "    else:\n",
    "        print( 'maximum iteration reached' )\n",
    "\n",
    "    # Compute the final segmented fit:\n",
    "    Xsolution = np.insert( np.append( breakpoints, max(X) ), 0, min(X) )\n",
    "    ones =  np.ones_like(Xsolution)\n",
    "    Rk = [ c*ramp( Xsolution - x0 ) for x0, c in zip(breakpoints, ck) ]\n",
    "\n",
    "    Ysolution = a*ones + b*Xsolution + np.sum( Rk, axis=0 )\n",
    "\n",
    "    return Xsolution, Ysolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.linspace( 0, 10, 27 )\n",
    "Y = 0.2*X  - 0.3* ramp(X-2) + 0.3*ramp(X-6) + 0.05*np.random.randn(len(X))\n",
    "plt.plot( X, Y, 'ok' );\n",
    "\n",
    "initialBreakpoints = [1,7]\n",
    "#Try also with 3 breakpoints, e.g. [3,5,6]\n",
    "plt.plot( *SegmentedLinearReg( X, Y, initialBreakpoints ), '-r' );\n",
    "plt.xlabel('X'); plt.ylabel('Y');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Descriptive statistics and dimensionality reduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12,svd_solver = 'full')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [
    {
     "data": {
      "text/plain": "PCA(n_components=12, svd_solver='full')"
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca.fit_transform(df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [
    {
     "data": {
      "text/plain": "B_Lag_0      3.739286e+00\nB_Lag_49     3.732974e+00\nB_Lag_48     3.732835e+00\nB_Lag_50     3.732389e+00\nB_Lag_51     3.730975e+00\n                 ...     \nD_Lag_227    3.577893e-30\nA_Lag_227    3.169344e-30\nC_Lag_225    0.000000e+00\nC_Lag_226    0.000000e+00\nC_Lag_227    0.000000e+00\nLength: 912, dtype: float64"
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.var(axis=0).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Variance for normal data: 858.6203648832986\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Variance for normal data: {pd.Series(df.var(axis=0)).sum()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Variance for PCA data: 816.107530042386\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Variance for PCA data: {pca_df.var(axis=0).sum()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [],
   "source": [
    "## 1.2 PCA Calulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Steps\n",
    "1. Centre Matrix\n",
    "2. Calculate the covariance matrix\n",
    "3. Calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "4. Select the first n eigenvectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [],
   "source": [
    "mean_df = np.mean(df.T,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "outputs": [],
   "source": [
    "centre =df - mean_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [],
   "source": [
    "covariance = np.cov(df.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "outputs": [
    {
     "data": {
      "text/plain": "      A_Lag_0   A_Lag_1   A_Lag_2   A_Lag_3   A_Lag_4   A_Lag_5   A_Lag_6  \\\n0    0.467807  0.461798  0.455921  0.450351  0.445263  0.440482  0.435965   \n1    0.397807  0.461798  0.455921  0.450351  0.445263  0.440482  0.435965   \n2    0.347807  0.391798  0.455921  0.450351  0.445263  0.440482  0.435965   \n3    0.327807  0.341798  0.385921  0.450351  0.445263  0.440482  0.435965   \n4    0.277807  0.321798  0.335921  0.380351  0.445263  0.440482  0.435965   \n..        ...       ...       ...       ...       ...       ...       ...   \n223 -0.622193 -0.568202 -0.534079 -0.399649 -0.374737 -0.179518 -0.124035   \n224 -0.692193 -0.628202 -0.574079 -0.539649 -0.404737 -0.379518 -0.184035   \n225 -0.802193 -0.698202 -0.634079 -0.579649 -0.544737 -0.409518 -0.384035   \n226 -0.872193 -0.808202 -0.704079 -0.639649 -0.584737 -0.549518 -0.414035   \n227 -0.902193 -0.878202 -0.814079 -0.709649 -0.644737 -0.589518 -0.554035   \n\n      A_Lag_7   A_Lag_8   A_Lag_9  ...  D_Lag_218  D_Lag_219  D_Lag_220  \\\n0    0.431623  0.427895  0.424298  ...   -0.00614  -0.004386   -0.00307   \n1    0.431623  0.427895  0.424298  ...   -0.00614  -0.004386   -0.00307   \n2    0.431623  0.427895  0.424298  ...   -0.00614  -0.004386   -0.00307   \n3    0.431623  0.427895  0.424298  ...   -0.00614  -0.004386   -0.00307   \n4    0.431623  0.427895  0.424298  ...   -0.00614  -0.004386   -0.00307   \n..        ...       ...       ...  ...        ...        ...        ...   \n223 -0.038377  0.047895  0.174298  ...    0.09386   0.295614    0.09693   \n224 -0.128377 -0.042105  0.044298  ...    0.09386   0.095614    0.29693   \n225 -0.188377 -0.132105 -0.045702  ...    0.19386   0.095614    0.09693   \n226 -0.388377 -0.192105 -0.135702  ...    0.29386   0.195614    0.09693   \n227 -0.418377 -0.392105 -0.195702  ...    0.39386   0.295614    0.19693   \n\n     D_Lag_221  D_Lag_222  D_Lag_223     D_Lag_224  D_Lag_225     D_Lag_226  \\\n0    -0.002193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n1    -0.002193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n2    -0.002193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n3    -0.002193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n4    -0.002193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n..         ...        ...        ...           ...        ...           ...   \n223  -0.102193  -0.001754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n224   0.097807  -0.101754  -0.001316 -1.776357e-15   0.000439 -1.887379e-15   \n225   0.297807   0.098246  -0.101316 -1.776357e-15   0.000439 -1.887379e-15   \n226   0.097807   0.298246   0.098684 -1.000000e-01   0.000439 -1.887379e-15   \n227   0.097807   0.098246   0.298684  1.000000e-01  -0.099561 -1.887379e-15   \n\n        D_Lag_227  \n0   -1.887379e-15  \n1   -1.887379e-15  \n2   -1.887379e-15  \n3   -1.887379e-15  \n4   -1.887379e-15  \n..            ...  \n223 -1.887379e-15  \n224 -1.887379e-15  \n225 -1.887379e-15  \n226 -1.887379e-15  \n227 -1.887379e-15  \n\n[228 rows x 912 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A_Lag_0</th>\n      <th>A_Lag_1</th>\n      <th>A_Lag_2</th>\n      <th>A_Lag_3</th>\n      <th>A_Lag_4</th>\n      <th>A_Lag_5</th>\n      <th>A_Lag_6</th>\n      <th>A_Lag_7</th>\n      <th>A_Lag_8</th>\n      <th>A_Lag_9</th>\n      <th>...</th>\n      <th>D_Lag_218</th>\n      <th>D_Lag_219</th>\n      <th>D_Lag_220</th>\n      <th>D_Lag_221</th>\n      <th>D_Lag_222</th>\n      <th>D_Lag_223</th>\n      <th>D_Lag_224</th>\n      <th>D_Lag_225</th>\n      <th>D_Lag_226</th>\n      <th>D_Lag_227</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.467807</td>\n      <td>0.461798</td>\n      <td>0.455921</td>\n      <td>0.450351</td>\n      <td>0.445263</td>\n      <td>0.440482</td>\n      <td>0.435965</td>\n      <td>0.431623</td>\n      <td>0.427895</td>\n      <td>0.424298</td>\n      <td>...</td>\n      <td>-0.00614</td>\n      <td>-0.004386</td>\n      <td>-0.00307</td>\n      <td>-0.002193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.397807</td>\n      <td>0.461798</td>\n      <td>0.455921</td>\n      <td>0.450351</td>\n      <td>0.445263</td>\n      <td>0.440482</td>\n      <td>0.435965</td>\n      <td>0.431623</td>\n      <td>0.427895</td>\n      <td>0.424298</td>\n      <td>...</td>\n      <td>-0.00614</td>\n      <td>-0.004386</td>\n      <td>-0.00307</td>\n      <td>-0.002193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.347807</td>\n      <td>0.391798</td>\n      <td>0.455921</td>\n      <td>0.450351</td>\n      <td>0.445263</td>\n      <td>0.440482</td>\n      <td>0.435965</td>\n      <td>0.431623</td>\n      <td>0.427895</td>\n      <td>0.424298</td>\n      <td>...</td>\n      <td>-0.00614</td>\n      <td>-0.004386</td>\n      <td>-0.00307</td>\n      <td>-0.002193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.327807</td>\n      <td>0.341798</td>\n      <td>0.385921</td>\n      <td>0.450351</td>\n      <td>0.445263</td>\n      <td>0.440482</td>\n      <td>0.435965</td>\n      <td>0.431623</td>\n      <td>0.427895</td>\n      <td>0.424298</td>\n      <td>...</td>\n      <td>-0.00614</td>\n      <td>-0.004386</td>\n      <td>-0.00307</td>\n      <td>-0.002193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.277807</td>\n      <td>0.321798</td>\n      <td>0.335921</td>\n      <td>0.380351</td>\n      <td>0.445263</td>\n      <td>0.440482</td>\n      <td>0.435965</td>\n      <td>0.431623</td>\n      <td>0.427895</td>\n      <td>0.424298</td>\n      <td>...</td>\n      <td>-0.00614</td>\n      <td>-0.004386</td>\n      <td>-0.00307</td>\n      <td>-0.002193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>-0.622193</td>\n      <td>-0.568202</td>\n      <td>-0.534079</td>\n      <td>-0.399649</td>\n      <td>-0.374737</td>\n      <td>-0.179518</td>\n      <td>-0.124035</td>\n      <td>-0.038377</td>\n      <td>0.047895</td>\n      <td>0.174298</td>\n      <td>...</td>\n      <td>0.09386</td>\n      <td>0.295614</td>\n      <td>0.09693</td>\n      <td>-0.102193</td>\n      <td>-0.001754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>-0.692193</td>\n      <td>-0.628202</td>\n      <td>-0.574079</td>\n      <td>-0.539649</td>\n      <td>-0.404737</td>\n      <td>-0.379518</td>\n      <td>-0.184035</td>\n      <td>-0.128377</td>\n      <td>-0.042105</td>\n      <td>0.044298</td>\n      <td>...</td>\n      <td>0.09386</td>\n      <td>0.095614</td>\n      <td>0.29693</td>\n      <td>0.097807</td>\n      <td>-0.101754</td>\n      <td>-0.001316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>-0.802193</td>\n      <td>-0.698202</td>\n      <td>-0.634079</td>\n      <td>-0.579649</td>\n      <td>-0.544737</td>\n      <td>-0.409518</td>\n      <td>-0.384035</td>\n      <td>-0.188377</td>\n      <td>-0.132105</td>\n      <td>-0.045702</td>\n      <td>...</td>\n      <td>0.19386</td>\n      <td>0.095614</td>\n      <td>0.09693</td>\n      <td>0.297807</td>\n      <td>0.098246</td>\n      <td>-0.101316</td>\n      <td>-1.776357e-15</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>-0.872193</td>\n      <td>-0.808202</td>\n      <td>-0.704079</td>\n      <td>-0.639649</td>\n      <td>-0.584737</td>\n      <td>-0.549518</td>\n      <td>-0.414035</td>\n      <td>-0.388377</td>\n      <td>-0.192105</td>\n      <td>-0.135702</td>\n      <td>...</td>\n      <td>0.29386</td>\n      <td>0.195614</td>\n      <td>0.09693</td>\n      <td>0.097807</td>\n      <td>0.298246</td>\n      <td>0.098684</td>\n      <td>-1.000000e-01</td>\n      <td>0.000439</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>-0.902193</td>\n      <td>-0.878202</td>\n      <td>-0.814079</td>\n      <td>-0.709649</td>\n      <td>-0.644737</td>\n      <td>-0.589518</td>\n      <td>-0.554035</td>\n      <td>-0.418377</td>\n      <td>-0.392105</td>\n      <td>-0.195702</td>\n      <td>...</td>\n      <td>0.39386</td>\n      <td>0.295614</td>\n      <td>0.19693</td>\n      <td>0.097807</td>\n      <td>0.098246</td>\n      <td>0.298684</td>\n      <td>1.000000e-01</td>\n      <td>-0.099561</td>\n      <td>-1.887379e-15</td>\n      <td>-1.887379e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>228 rows × 912 columns</p>\n</div>"
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centre"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.61888151e+00, 1.61379207e+00, 1.60351657e+00, ...,\n        3.97441842e-04, 0.00000000e+00, 0.00000000e+00],\n       [1.61379207e+00, 1.61622362e+00, 1.61121794e+00, ...,\n        3.86873020e-04, 0.00000000e+00, 0.00000000e+00],\n       [1.60351657e+00, 1.61121794e+00, 1.61373087e+00, ...,\n        3.58625087e-04, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [3.97441842e-04, 3.86873020e-04, 3.58625087e-04, ...,\n        4.38596491e-05, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "outputs": [],
   "source": [
    "eigenvalues,eigenvectors = np.linalg.eig(covariance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "outputs": [],
   "source": [
    "idx = eigenvalues.argsort()[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "outputs": [],
   "source": [
    "eigenvalues = eigenvalues[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "outputs": [],
   "source": [
    "eigenvectors = eigenvectors[:,idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "outputs": [],
   "source": [
    "principals = eigenvectors.T.dot(centre.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.06210427e+00+0.j, -6.26431188e+00+0.j, -1.27266099e+00+0.j,\n         3.57213942e+00+0.j],\n       [ 8.06542003e+00+0.j, -6.26986619e+00+0.j, -1.27365121e+00+0.j,\n         3.57383791e+00+0.j],\n       [ 8.06781954e+00+0.j, -6.27823640e+00+0.j, -1.27934771e+00+0.j,\n         3.57854388e+00+0.j],\n       [ 8.04007376e+00+0.j, -6.28042016e+00+0.j, -1.28182733e+00+0.j,\n         3.56158098e+00+0.j],\n       [ 8.02199576e+00+0.j, -6.28751713e+00+0.j, -1.27569632e+00+0.j,\n         3.54146479e+00+0.j],\n       [ 7.99932263e+00+0.j, -6.29550273e+00+0.j, -1.27683411e+00+0.j,\n         3.52589443e+00+0.j],\n       [ 7.97731555e+00+0.j, -6.33599497e+00+0.j, -1.27872681e+00+0.j,\n         3.50757504e+00+0.j],\n       [ 7.96230374e+00+0.j, -6.38741949e+00+0.j, -1.27504445e+00+0.j,\n         3.48718436e+00+0.j],\n       [ 7.95512628e+00+0.j, -6.45034432e+00+0.j, -1.26531551e+00+0.j,\n         3.46559884e+00+0.j],\n       [ 7.95284818e+00+0.j, -6.56518255e+00+0.j, -1.25074795e+00+0.j,\n         3.43854096e+00+0.j],\n       [ 7.99730788e+00+0.j, -6.70737096e+00+0.j, -1.22279727e+00+0.j,\n         3.42809135e+00+0.j],\n       [ 8.05700435e+00+0.j, -6.86393073e+00+0.j, -1.18221646e+00+0.j,\n         3.41825852e+00+0.j],\n       [ 8.13383151e+00+0.j, -7.03610590e+00+0.j, -1.12863694e+00+0.j,\n         3.41147752e+00+0.j],\n       [ 8.24606851e+00+0.j, -7.22919398e+00+0.j, -1.05882834e+00+0.j,\n         3.41770337e+00+0.j],\n       [ 8.38731418e+00+0.j, -7.44035066e+00+0.j, -9.80763418e-01+0.j,\n         3.44287682e+00+0.j],\n       [ 8.56128650e+00+0.j, -7.66991886e+00+0.j, -8.90485110e-01+0.j,\n         3.48702552e+00+0.j],\n       [ 8.73343967e+00+0.j, -7.91036665e+00+0.j, -8.06565645e-01+0.j,\n         3.54948358e+00+0.j],\n       [ 8.95632168e+00+0.j, -8.16121884e+00+0.j, -7.07585729e-01+0.j,\n         3.64253939e+00+0.j],\n       [ 9.20063124e+00+0.j, -8.39659141e+00+0.j, -5.90673966e-01+0.j,\n         3.74939912e+00+0.j],\n       [ 9.45561024e+00+0.j, -8.61201933e+00+0.j, -4.69207075e-01+0.j,\n         3.87811682e+00+0.j],\n       [ 9.73521762e+00+0.j, -8.81140607e+00+0.j, -3.46662720e-01+0.j,\n         4.04004099e+00+0.j],\n       [ 1.00576838e+01+0.j, -9.01252732e+00+0.j, -2.06375413e-01+0.j,\n         4.22665804e+00+0.j],\n       [ 1.03878947e+01+0.j, -9.19944712e+00+0.j, -6.79418084e-02+0.j,\n         4.43708804e+00+0.j],\n       [ 1.07290527e+01+0.j, -9.37214855e+00+0.j,  7.04553580e-02+0.j,\n         4.66938780e+00+0.j],\n       [ 1.10773986e+01+0.j, -9.53515651e+00+0.j,  2.02046120e-01+0.j,\n         4.92460282e+00+0.j],\n       [ 1.14128391e+01+0.j, -9.67329670e+00+0.j,  3.05103966e-01+0.j,\n         5.20971714e+00+0.j],\n       [ 1.17517722e+01+0.j, -9.79091267e+00+0.j,  3.96820730e-01+0.j,\n         5.51240053e+00+0.j],\n       [ 1.21023681e+01+0.j, -9.85770607e+00+0.j,  4.84254212e-01+0.j,\n         5.82982215e+00+0.j],\n       [ 1.24502999e+01+0.j, -9.88690038e+00+0.j,  5.73554372e-01+0.j,\n         6.14152401e+00+0.j],\n       [ 1.28126516e+01+0.j, -9.88536755e+00+0.j,  6.60026101e-01+0.j,\n         6.45674593e+00+0.j],\n       [ 1.31662396e+01+0.j, -9.85051087e+00+0.j,  7.16653588e-01+0.j,\n         6.78257099e+00+0.j],\n       [ 1.34830881e+01+0.j, -9.77128062e+00+0.j,  7.30177087e-01+0.j,\n         7.10829365e+00+0.j],\n       [ 1.37498458e+01+0.j, -9.64237384e+00+0.j,  7.03709988e-01+0.j,\n         7.41503142e+00+0.j],\n       [ 1.40077554e+01+0.j, -9.46864877e+00+0.j,  6.44452911e-01+0.j,\n         7.71358832e+00+0.j],\n       [ 1.42209417e+01+0.j, -9.24239468e+00+0.j,  5.49319998e-01+0.j,\n         7.97847027e+00+0.j],\n       [ 1.44201933e+01+0.j, -8.97864177e+00+0.j,  4.13219877e-01+0.j,\n         8.22502558e+00+0.j],\n       [ 1.46185165e+01+0.j, -8.65886960e+00+0.j,  2.47506749e-01+0.j,\n         8.44503620e+00+0.j],\n       [ 1.48227486e+01+0.j, -8.31168012e+00+0.j,  6.65643636e-02+0.j,\n         8.62030182e+00+0.j],\n       [ 1.50139837e+01+0.j, -7.95499209e+00+0.j, -1.55481077e-01+0.j,\n         8.75730003e+00+0.j],\n       [ 1.51913798e+01+0.j, -7.58720122e+00+0.j, -4.12441545e-01+0.j,\n         8.84588595e+00+0.j],\n       [ 1.53536266e+01+0.j, -7.20676950e+00+0.j, -7.06922417e-01+0.j,\n         8.88433223e+00+0.j],\n       [ 1.53987127e+01+0.j, -6.77877151e+00+0.j, -1.05606198e+00+0.j,\n         8.82795650e+00+0.j],\n       [ 1.55168053e+01+0.j, -6.38045214e+00+0.j, -1.42897869e+00+0.j,\n         8.75438696e+00+0.j],\n       [ 1.56255777e+01+0.j, -5.98266996e+00+0.j, -1.82291748e+00+0.j,\n         8.61341444e+00+0.j],\n       [ 1.57260022e+01+0.j, -5.58933565e+00+0.j, -2.23376087e+00+0.j,\n         8.40290909e+00+0.j],\n       [ 1.58160784e+01+0.j, -5.19144538e+00+0.j, -2.66016233e+00+0.j,\n         8.12494635e+00+0.j],\n       [ 1.59014449e+01+0.j, -4.79946000e+00+0.j, -3.08904749e+00+0.j,\n         7.77441356e+00+0.j],\n       [ 1.59362655e+01+0.j, -4.39794585e+00+0.j, -3.52697865e+00+0.j,\n         7.33760760e+00+0.j],\n       [ 1.59610050e+01+0.j, -3.98990854e+00+0.j, -3.96251389e+00+0.j,\n         6.83588217e+00+0.j],\n       [ 1.59689382e+01+0.j, -3.59057048e+00+0.j, -4.39919102e+00+0.j,\n         6.27516235e+00+0.j],\n       [ 1.59564271e+01+0.j, -3.19763665e+00+0.j, -4.81233759e+00+0.j,\n         5.63912420e+00+0.j],\n       [ 1.59358782e+01+0.j, -2.80975982e+00+0.j, -5.20527764e+00+0.j,\n         4.94970894e+00+0.j],\n       [ 1.58932273e+01+0.j, -2.44239768e+00+0.j, -5.58476106e+00+0.j,\n         4.21442071e+00+0.j],\n       [ 1.57894202e+01+0.j, -2.07984994e+00+0.j, -5.94875892e+00+0.j,\n         3.42030038e+00+0.j],\n       [ 1.56820721e+01+0.j, -1.75569519e+00+0.j, -6.26730450e+00+0.j,\n         2.58100372e+00+0.j],\n       [ 1.55617399e+01+0.j, -1.46379017e+00+0.j, -6.54581624e+00+0.j,\n         1.71015457e+00+0.j],\n       [ 1.54339422e+01+0.j, -1.20599576e+00+0.j, -6.76995284e+00+0.j,\n         8.10657701e-01+0.j],\n       [ 1.52981609e+01+0.j, -1.00370494e+00+0.j, -6.93534786e+00+0.j,\n        -1.11219152e-01+0.j],\n       [ 1.51484503e+01+0.j, -8.44723170e-01+0.j, -7.04375598e+00+0.j,\n        -1.04227244e+00+0.j],\n       [ 1.49927196e+01+0.j, -7.28412149e-01+0.j, -7.07749388e+00+0.j,\n        -1.98075820e+00+0.j],\n       [ 1.48148082e+01+0.j, -6.59561998e-01+0.j, -7.04877337e+00+0.j,\n        -2.91205721e+00+0.j],\n       [ 1.46219619e+01+0.j, -6.33812616e-01+0.j, -6.95524211e+00+0.j,\n        -3.82259646e+00+0.j],\n       [ 1.44127013e+01+0.j, -6.48105750e-01+0.j, -6.79408647e+00+0.j,\n        -4.70417287e+00+0.j],\n       [ 1.41994288e+01+0.j, -7.05729968e-01+0.j, -6.55208674e+00+0.j,\n        -5.55244118e+00+0.j],\n       [ 1.39949250e+01+0.j, -8.08280604e-01+0.j, -6.21663046e+00+0.j,\n        -6.36269967e+00+0.j],\n       [ 1.38036828e+01+0.j, -9.35443131e-01+0.j, -5.78742414e+00+0.j,\n        -7.12215425e+00+0.j],\n       [ 1.36070863e+01+0.j, -1.06840470e+00+0.j, -5.28570689e+00+0.j,\n        -7.81250696e+00+0.j],\n       [ 1.34038657e+01+0.j, -1.20813035e+00+0.j, -4.71519571e+00+0.j,\n        -8.42681228e+00+0.j],\n       [ 1.31988358e+01+0.j, -1.34965144e+00+0.j, -4.07157272e+00+0.j,\n        -8.96276269e+00+0.j],\n       [ 1.29816904e+01+0.j, -1.48234035e+00+0.j, -3.37707573e+00+0.j,\n        -9.40410477e+00+0.j],\n       [ 1.27715458e+01+0.j, -1.61132557e+00+0.j, -2.61421954e+00+0.j,\n        -9.75902370e+00+0.j],\n       [ 1.25552920e+01+0.j, -1.73133675e+00+0.j, -1.81150704e+00+0.j,\n        -1.00116344e+01+0.j],\n       [ 1.23426279e+01+0.j, -1.83885178e+00+0.j, -9.64931213e-01+0.j,\n        -1.01662024e+01+0.j],\n       [ 1.21130489e+01+0.j, -1.92738229e+00+0.j, -1.03291533e-01+0.j,\n        -1.02146894e+01+0.j],\n       [ 1.18885574e+01+0.j, -1.99659168e+00+0.j,  7.87940254e-01+0.j,\n        -1.01690891e+01+0.j],\n       [ 1.16630531e+01+0.j, -2.02898778e+00+0.j,  1.69642249e+00+0.j,\n        -1.00299806e+01+0.j],\n       [ 1.14361307e+01+0.j, -2.03725510e+00+0.j,  2.60130104e+00+0.j,\n        -9.79322717e+00+0.j],\n       [ 1.12043823e+01+0.j, -2.01273253e+00+0.j,  3.49341204e+00+0.j,\n        -9.46511338e+00+0.j],\n       [ 1.09751419e+01+0.j, -1.98152929e+00+0.j,  4.36404769e+00+0.j,\n        -9.05277822e+00+0.j],\n       [ 1.07534701e+01+0.j, -1.92437565e+00+0.j,  5.20906854e+00+0.j,\n        -8.56327104e+00+0.j],\n       [ 1.05358810e+01+0.j, -1.83213811e+00+0.j,  6.02003956e+00+0.j,\n        -8.00648375e+00+0.j],\n       [ 1.03361273e+01+0.j, -1.70877958e+00+0.j,  6.79933926e+00+0.j,\n        -7.39425390e+00+0.j],\n       [ 1.01411087e+01+0.j, -1.55151718e+00+0.j,  7.51727149e+00+0.j,\n        -6.72333919e+00+0.j],\n       [ 9.96481450e+00+0.j, -1.36537234e+00+0.j,  8.16440107e+00+0.j,\n        -5.99325708e+00+0.j],\n       [ 9.79636198e+00+0.j, -1.16744361e+00+0.j,  8.73738514e+00+0.j,\n        -5.22731882e+00+0.j],\n       [ 9.64158454e+00+0.j, -9.40702957e-01+0.j,  9.23534681e+00+0.j,\n        -4.43620409e+00+0.j],\n       [ 9.51953654e+00+0.j, -6.94997965e-01+0.j,  9.65065635e+00+0.j,\n        -3.61713240e+00+0.j],\n       [ 9.40456075e+00+0.j, -4.31869840e-01+0.j,  9.96812280e+00+0.j,\n        -2.78500253e+00+0.j],\n       [ 9.31064129e+00+0.j, -1.49761234e-01+0.j,  1.02018474e+01+0.j,\n        -1.95665237e+00+0.j],\n       [ 9.25702223e+00+0.j,  1.48739074e-01+0.j,  1.03485141e+01+0.j,\n        -1.12649373e+00+0.j],\n       [ 9.22141518e+00+0.j,  4.80488394e-01+0.j,  1.04016370e+01+0.j,\n        -3.07313535e-01+0.j],\n       [ 9.21477576e+00+0.j,  8.32878828e-01+0.j,  1.03596441e+01+0.j,\n         5.01454289e-01+0.j],\n       [ 9.21723955e+00+0.j,  1.18633871e+00+0.j,  1.02200063e+01+0.j,\n         1.28388992e+00+0.j],\n       [ 9.23238103e+00+0.j,  1.52095256e+00+0.j,  9.97120474e+00+0.j,\n         2.04893383e+00+0.j],\n       [ 9.25254819e+00+0.j,  1.84407978e+00+0.j,  9.62722826e+00+0.j,\n         2.77770329e+00+0.j],\n       [ 9.31423045e+00+0.j,  2.15294424e+00+0.j,  9.21156004e+00+0.j,\n         3.47023900e+00+0.j],\n       [ 9.39190954e+00+0.j,  2.46081467e+00+0.j,  8.72430623e+00+0.j,\n         4.11646475e+00+0.j],\n       [ 9.47782205e+00+0.j,  2.75646848e+00+0.j,  8.16278525e+00+0.j,\n         4.71824771e+00+0.j],\n       [ 9.59287658e+00+0.j,  3.02636697e+00+0.j,  7.53982268e+00+0.j,\n         5.28044321e+00+0.j],\n       [ 9.72247893e+00+0.j,  3.27647980e+00+0.j,  6.86385427e+00+0.j,\n         5.79325097e+00+0.j],\n       [ 9.85589794e+00+0.j,  3.50389664e+00+0.j,  6.13941118e+00+0.j,\n         6.25502611e+00+0.j],\n       [ 1.00135243e+01+0.j,  3.69373742e+00+0.j,  5.37541441e+00+0.j,\n         6.67595200e+00+0.j],\n       [ 1.01683127e+01+0.j,  3.85240435e+00+0.j,  4.57850551e+00+0.j,\n         7.04383831e+00+0.j],\n       [ 1.03199194e+01+0.j,  3.96924629e+00+0.j,  3.75157189e+00+0.j,\n         7.36504566e+00+0.j],\n       [ 1.04660165e+01+0.j,  4.04235258e+00+0.j,  2.90274070e+00+0.j,\n         7.64002616e+00+0.j],\n       [ 1.06268094e+01+0.j,  4.09998474e+00+0.j,  2.06130430e+00+0.j,\n         7.86536374e+00+0.j],\n       [ 1.07958089e+01+0.j,  4.11307144e+00+0.j,  1.23365821e+00+0.j,\n         8.04079321e+00+0.j],\n       [ 1.09756226e+01+0.j,  4.08522012e+00+0.j,  4.36010726e-01+0.j,\n         8.16317847e+00+0.j],\n       [ 1.11465574e+01+0.j,  4.01328678e+00+0.j, -3.50196059e-01+0.j,\n         8.24793531e+00+0.j],\n       [ 1.13104142e+01+0.j,  3.89446316e+00+0.j, -1.11446161e+00+0.j,\n         8.29612267e+00+0.j],\n       [ 1.14657929e+01+0.j,  3.73045729e+00+0.j, -1.85120202e+00+0.j,\n         8.30938362e+00+0.j],\n       [ 1.16186610e+01+0.j,  3.58760245e+00+0.j, -2.54339576e+00+0.j,\n         8.29020672e+00+0.j],\n       [ 1.17421751e+01+0.j,  3.42048584e+00+0.j, -3.21600189e+00+0.j,\n         8.24673472e+00+0.j],\n       [ 1.18580520e+01+0.j,  3.23307725e+00+0.j, -3.83705603e+00+0.j,\n         8.16567719e+00+0.j],\n       [ 1.19858368e+01+0.j,  3.04816580e+00+0.j, -4.41048716e+00+0.j,\n         8.06669757e+00+0.j],\n       [ 1.20974418e+01+0.j,  2.85176796e+00+0.j, -4.93890161e+00+0.j,\n         7.93366133e+00+0.j],\n       [ 1.21760071e+01+0.j,  2.65231432e+00+0.j, -5.44236468e+00+0.j,\n         7.77453866e+00+0.j],\n       [ 1.21969068e+01+0.j,  2.57910382e+00+0.j, -5.91739795e+00+0.j,\n         7.57974228e+00+0.j],\n       [ 1.21366531e+01+0.j,  2.55441255e+00+0.j, -6.37952613e+00+0.j,\n         7.33450583e+00+0.j],\n       [ 1.19501615e+01+0.j,  2.59615764e+00+0.j, -6.86061973e+00+0.j,\n         7.03739576e+00+0.j],\n       [ 1.16864178e+01+0.j,  2.86442395e+00+0.j, -7.32760622e+00+0.j,\n         6.69665358e+00+0.j],\n       [ 1.13658592e+01+0.j,  3.17945937e+00+0.j, -7.79232634e+00+0.j,\n         6.30411742e+00+0.j],\n       [ 1.09702582e+01+0.j,  3.53325371e+00+0.j, -8.23750012e+00+0.j,\n         5.82253915e+00+0.j],\n       [ 1.05023602e+01+0.j,  3.91194455e+00+0.j, -8.69184680e+00+0.j,\n         5.26950397e+00+0.j],\n       [ 9.98916403e+00+0.j,  4.31265150e+00+0.j, -9.13224354e+00+0.j,\n         4.62752024e+00+0.j],\n       [ 9.43033339e+00+0.j,  4.73510284e+00+0.j, -9.57986937e+00+0.j,\n         3.90961435e+00+0.j],\n       [ 8.84124552e+00+0.j,  5.13671447e+00+0.j, -1.00145010e+01+0.j,\n         3.09621150e+00+0.j],\n       [ 8.21749122e+00+0.j,  5.55060636e+00+0.j, -1.04480314e+01+0.j,\n         2.19642700e+00+0.j],\n       [ 7.58933038e+00+0.j,  5.96840879e+00+0.j, -1.08399678e+01+0.j,\n         1.18831595e+00+0.j],\n       [ 6.93365777e+00+0.j,  6.27794802e+00+0.j, -1.12107877e+01+0.j,\n         6.90268806e-02+0.j],\n       [ 6.27914194e+00+0.j,  6.57890954e+00+0.j, -1.15352069e+01+0.j,\n        -1.15297701e+00+0.j],\n       [ 5.64529199e+00+0.j,  6.86310448e+00+0.j, -1.17911251e+01+0.j,\n        -2.47957921e+00+0.j],\n       [ 5.00582074e+00+0.j,  6.95608021e+00+0.j, -1.19859405e+01+0.j,\n        -3.92804622e+00+0.j],\n       [ 4.37628373e+00+0.j,  7.02459118e+00+0.j, -1.21155810e+01+0.j,\n        -5.45957580e+00+0.j],\n       [ 3.75750690e+00+0.j,  7.06273661e+00+0.j, -1.21764335e+01+0.j,\n        -7.06061263e+00+0.j],\n       [ 3.17506121e+00+0.j,  7.03587138e+00+0.j, -1.21224279e+01+0.j,\n        -8.73987278e+00+0.j],\n       [ 2.61076668e+00+0.j,  6.97747719e+00+0.j, -1.19750026e+01+0.j,\n        -1.04585395e+01+0.j],\n       [ 2.07012790e+00+0.j,  6.88206776e+00+0.j, -1.17221045e+01+0.j,\n        -1.21998917e+01+0.j],\n       [ 1.54557808e+00+0.j,  6.75491607e+00+0.j, -1.13685671e+01+0.j,\n        -1.39376595e+01+0.j],\n       [ 1.04859305e+00+0.j,  6.59446043e+00+0.j, -1.08910614e+01+0.j,\n        -1.56607776e+01+0.j],\n       [ 5.66901729e-01+0.j,  6.40339276e+00+0.j, -1.02953878e+01+0.j,\n        -1.73418946e+01+0.j],\n       [ 1.10498186e-01+0.j,  6.17518549e+00+0.j, -9.56016056e+00+0.j,\n        -1.89677098e+01+0.j],\n       [-3.27906902e-01+0.j,  5.90586605e+00+0.j, -8.69494227e+00+0.j,\n        -2.05061559e+01+0.j],\n       [-7.52012478e-01+0.j,  5.60128710e+00+0.j, -7.69766614e+00+0.j,\n        -2.19367957e+01+0.j],\n       [-1.15191660e+00+0.j,  5.25085971e+00+0.j, -6.55663682e+00+0.j,\n        -2.32419046e+01+0.j],\n       [-1.53595096e+00+0.j,  4.86110119e+00+0.j, -5.28173311e+00+0.j,\n        -2.43965031e+01+0.j],\n       [-1.90352383e+00+0.j,  4.45547817e+00+0.j, -3.87664261e+00+0.j,\n        -2.53790695e+01+0.j],\n       [-2.22149321e+00+0.j,  4.07675166e+00+0.j, -2.33797095e+00+0.j,\n        -2.61597958e+01+0.j],\n       [-2.53048482e+00+0.j,  3.70760877e+00+0.j, -6.87458794e-01+0.j,\n        -2.67369627e+01+0.j],\n       [-2.83509657e+00+0.j,  3.35674437e+00+0.j,  1.05284910e+00+0.j,\n        -2.70933736e+01+0.j],\n       [-3.11891413e+00+0.j,  3.03093428e+00+0.j,  2.87576967e+00+0.j,\n        -2.72151384e+01+0.j],\n       [-3.40554522e+00+0.j,  2.74574439e+00+0.j,  4.75830514e+00+0.j,\n        -2.71006873e+01+0.j],\n       [-3.69562558e+00+0.j,  2.51059551e+00+0.j,  6.68148489e+00+0.j,\n        -2.67466336e+01+0.j],\n       [-3.96492639e+00+0.j,  2.37093564e+00+0.j,  8.65101721e+00+0.j,\n        -2.61586520e+01+0.j],\n       [-4.25640440e+00+0.j,  2.31194132e+00+0.j,  1.06200935e+01+0.j,\n        -2.53439111e+01+0.j],\n       [-4.57042885e+00+0.j,  2.33973798e+00+0.j,  1.25661860e+01+0.j,\n        -2.43081349e+01+0.j],\n       [-4.88744983e+00+0.j,  2.49790285e+00+0.j,  1.44610296e+01+0.j,\n        -2.30405459e+01+0.j],\n       [-5.21942653e+00+0.j,  2.75258705e+00+0.j,  1.62841084e+01+0.j,\n        -2.15642279e+01+0.j],\n       [-5.56287584e+00+0.j,  3.10491306e+00+0.j,  1.80154030e+01+0.j,\n        -1.98960694e+01+0.j],\n       [-5.91691690e+00+0.j,  3.55421956e+00+0.j,  1.96312642e+01+0.j,\n        -1.80530032e+01+0.j],\n       [-6.28555184e+00+0.j,  4.10100178e+00+0.j,  2.11051868e+01+0.j,\n        -1.60535128e+01+0.j],\n       [-6.67493264e+00+0.j,  4.74746527e+00+0.j,  2.24121499e+01+0.j,\n        -1.39202277e+01+0.j],\n       [-7.09601368e+00+0.j,  5.49466249e+00+0.j,  2.35374042e+01+0.j,\n        -1.16906297e+01+0.j],\n       [-7.52864262e+00+0.j,  6.32600520e+00+0.j,  2.44640557e+01+0.j,\n        -9.38471859e+00+0.j],\n       [-7.96822015e+00+0.j,  7.23865933e+00+0.j,  2.51850826e+01+0.j,\n        -7.03654654e+00+0.j],\n       [-8.42402963e+00+0.j,  8.23322520e+00+0.j,  2.56796690e+01+0.j,\n        -4.67342460e+00+0.j],\n       [-8.89628076e+00+0.j,  9.30205614e+00+0.j,  2.59316191e+01+0.j,\n        -2.32318706e+00+0.j],\n       [-9.38725880e+00+0.j,  1.04343620e+01+0.j,  2.59232418e+01+0.j,\n        -1.09591920e-02+0.j],\n       [-9.89122070e+00+0.j,  1.16326196e+01+0.j,  2.56631757e+01+0.j,\n         2.22386430e+00+0.j],\n       [-1.04032643e+01+0.j,  1.28858370e+01+0.j,  2.51379250e+01+0.j,\n         4.36021411e+00+0.j],\n       [-1.09192170e+01+0.j,  1.41614102e+01+0.j,  2.43516328e+01+0.j,\n         6.36728760e+00+0.j],\n       [-1.14465880e+01+0.j,  1.54131503e+01+0.j,  2.33050476e+01+0.j,\n         8.21615322e+00+0.j],\n       [-1.20060823e+01+0.j,  1.66690598e+01+0.j,  2.19943228e+01+0.j,\n         9.88490958e+00+0.j],\n       [-1.25521807e+01+0.j,  1.78965755e+01+0.j,  2.04628374e+01+0.j,\n         1.13493510e+01+0.j],\n       [-1.30867333e+01+0.j,  1.90707037e+01+0.j,  1.87276508e+01+0.j,\n         1.25916135e+01+0.j],\n       [-1.36121835e+01+0.j,  2.01859370e+01+0.j,  1.68009847e+01+0.j,\n         1.36063183e+01+0.j],\n       [-1.41421762e+01+0.j,  2.12318417e+01+0.j,  1.46945277e+01+0.j,\n         1.43894584e+01+0.j],\n       [-1.46710843e+01+0.j,  2.21645392e+01+0.j,  1.24359219e+01+0.j,\n         1.49356249e+01+0.j],\n       [-1.52262633e+01+0.j,  2.29944878e+01+0.j,  1.00450368e+01+0.j,\n         1.52410835e+01+0.j],\n       [-1.57675810e+01+0.j,  2.36901930e+01+0.j,  7.57778690e+00+0.j,\n         1.53070414e+01+0.j],\n       [-1.63149504e+01+0.j,  2.42129198e+01+0.j,  5.05377783e+00+0.j,\n         1.51413216e+01+0.j],\n       [-1.68589892e+01+0.j,  2.45713986e+01+0.j,  2.51100806e+00+0.j,\n         1.47611415e+01+0.j],\n       [-1.74001656e+01+0.j,  2.47592734e+01+0.j, -2.29749329e-02+0.j,\n         1.41867490e+01+0.j],\n       [-1.79426271e+01+0.j,  2.47781327e+01+0.j, -2.52065554e+00+0.j,\n         1.34385215e+01+0.j],\n       [-1.84780857e+01+0.j,  2.46137851e+01+0.j, -4.93465015e+00+0.j,\n         1.25278610e+01+0.j],\n       [-1.90242230e+01+0.j,  2.42629180e+01+0.j, -7.25435814e+00+0.j,\n         1.14844066e+01+0.j],\n       [-1.95806717e+01+0.j,  2.37155739e+01+0.j, -9.44126268e+00+0.j,\n         1.03229122e+01+0.j],\n       [-2.01425398e+01+0.j,  2.29651116e+01+0.j, -1.14755349e+01+0.j,\n         9.07705256e+00+0.j],\n       [-2.07207035e+01+0.j,  2.20254698e+01+0.j, -1.33294724e+01+0.j,\n         7.76308923e+00+0.j],\n       [-2.13095053e+01+0.j,  2.09059528e+01+0.j, -1.49864549e+01+0.j,\n         6.41286221e+00+0.j],\n       [-2.19024248e+01+0.j,  1.96118251e+01+0.j, -1.64214354e+01+0.j,\n         5.04873135e+00+0.j],\n       [-2.24968602e+01+0.j,  1.81430932e+01+0.j, -1.76315452e+01+0.j,\n         3.70510588e+00+0.j],\n       [-2.31081924e+01+0.j,  1.65134955e+01+0.j, -1.86232984e+01+0.j,\n         2.41052068e+00+0.j],\n       [-2.37378772e+01+0.j,  1.47380337e+01+0.j, -1.93842689e+01+0.j,\n         1.18327190e+00+0.j],\n       [-2.43601193e+01+0.j,  1.28256435e+01+0.j, -1.98832930e+01+0.j,\n         2.98197317e-02+0.j],\n       [-2.49714671e+01+0.j,  1.07905419e+01+0.j, -2.01259889e+01+0.j,\n        -1.02681204e+00+0.j],\n       [-2.55697538e+01+0.j,  8.65670268e+00+0.j, -2.01109960e+01+0.j,\n        -1.97429336e+00+0.j],\n       [-2.61485746e+01+0.j,  6.44331082e+00+0.j, -1.98412923e+01+0.j,\n        -2.79982086e+00+0.j],\n       [-2.67128507e+01+0.j,  4.17782578e+00+0.j, -1.93493894e+01+0.j,\n        -3.47629319e+00+0.j],\n       [-2.72624829e+01+0.j,  1.88203860e+00+0.j, -1.86441325e+01+0.j,\n        -4.00149366e+00+0.j],\n       [-2.78019649e+01+0.j, -4.17686468e-01+0.j, -1.77449753e+01+0.j,\n        -4.37002420e+00+0.j],\n       [-2.83332341e+01+0.j, -2.69576822e+00+0.j, -1.66750731e+01+0.j,\n        -4.57975817e+00+0.j],\n       [-2.88318713e+01+0.j, -4.93998724e+00+0.j, -1.54421917e+01+0.j,\n        -4.63778592e+00+0.j],\n       [-2.93168409e+01+0.j, -7.11886146e+00+0.j, -1.40876537e+01+0.j,\n        -4.54070055e+00+0.j],\n       [-2.97641893e+01+0.j, -9.22223166e+00+0.j, -1.26162674e+01+0.j,\n        -4.30314993e+00+0.j],\n       [-3.01760204e+01+0.j, -1.12145505e+01+0.j, -1.10619921e+01+0.j,\n        -3.93384786e+00+0.j],\n       [-3.05635213e+01+0.j, -1.30786247e+01+0.j, -9.46775592e+00+0.j,\n        -3.43625511e+00+0.j],\n       [-3.09065592e+01+0.j, -1.48037870e+01+0.j, -7.83675751e+00+0.j,\n        -2.83288224e+00+0.j],\n       [-3.12133296e+01+0.j, -1.63783993e+01+0.j, -6.21170917e+00+0.j,\n        -2.13221566e+00+0.j],\n       [-3.14653510e+01+0.j, -1.77965694e+01+0.j, -4.59529040e+00+0.j,\n        -1.36027675e+00+0.j],\n       [-3.16584105e+01+0.j, -1.90546167e+01+0.j, -3.01487817e+00+0.j,\n        -5.28943436e-01+0.j],\n       [-3.17876127e+01+0.j, -2.01377207e+01+0.j, -1.48767758e+00+0.j,\n         3.38288270e-01+0.j],\n       [-3.18598174e+01+0.j, -2.10373753e+01+0.j, -3.98383852e-02+0.j,\n         1.22724874e+00+0.j],\n       [-3.18649311e+01+0.j, -2.17709357e+01+0.j,  1.31909462e+00+0.j,\n         2.11807460e+00+0.j],\n       [-3.18077659e+01+0.j, -2.23189061e+01+0.j,  2.57124086e+00+0.j,\n         2.99085394e+00+0.j],\n       [-3.16779021e+01+0.j, -2.26932295e+01+0.j,  3.70175710e+00+0.j,\n         3.83389446e+00+0.j],\n       [-3.14620585e+01+0.j, -2.28935348e+01+0.j,  4.71859708e+00+0.j,\n         4.62524977e+00+0.j],\n       [-3.11500285e+01+0.j, -2.29215073e+01+0.j,  5.61450641e+00+0.j,\n         5.34472105e+00+0.j],\n       [-3.07536102e+01+0.j, -2.27794651e+01+0.j,  6.37405097e+00+0.j,\n         5.98732219e+00+0.j],\n       [-3.02961004e+01+0.j, -2.24913307e+01+0.j,  6.96101887e+00+0.j,\n         6.56216591e+00+0.j],\n       [-2.97554418e+01+0.j, -2.20570100e+01+0.j,  7.41611507e+00+0.j,\n         7.04172945e+00+0.j],\n       [-2.91597372e+01+0.j, -2.14882025e+01+0.j,  7.70499725e+00+0.j,\n         7.44221383e+00+0.j],\n       [-2.85050902e+01+0.j, -2.08007792e+01+0.j,  7.85171606e+00+0.j,\n         7.75159535e+00+0.j],\n       [-2.77947999e+01+0.j, -2.00183656e+01+0.j,  7.87046387e+00+0.j,\n         7.97167746e+00+0.j],\n       [-2.70255141e+01+0.j, -1.91590146e+01+0.j,  7.77873003e+00+0.j,\n         8.10097946e+00+0.j],\n       [-2.62044003e+01+0.j, -1.82415157e+01+0.j,  7.58382827e+00+0.j,\n         8.14784854e+00+0.j],\n       [-2.53550374e+01+0.j, -1.72753480e+01+0.j,  7.29161560e+00+0.j,\n         8.13059988e+00+0.j],\n       [-2.44767166e+01+0.j, -1.62787583e+01+0.j,  6.92806963e+00+0.j,\n         8.05114090e+00+0.j]])"
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principals.T[:,0:4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.28500293, 0.12987233, 0.11763366, 0.11256116, 0.09580389,\n       0.08312435, 0.03927419, 0.02808612, 0.01750836, 0.01535912,\n       0.01426452, 0.0119964 ])"
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 4.89692564e-02,  5.05804607e-02,  5.21504048e-02, ...,\n        -4.40632771e-05,  0.00000000e+00,  0.00000000e+00],\n       [ 6.25639856e-02,  6.09702010e-02,  5.92217987e-02, ...,\n         6.43098664e-05,  0.00000000e+00,  0.00000000e+00],\n       [ 1.34536337e-02,  1.29099794e-02,  1.23410151e-02, ...,\n        -3.02171567e-05, -0.00000000e+00, -0.00000000e+00],\n       ...,\n       [ 1.17643967e-02,  9.86154777e-03,  8.30500647e-03, ...,\n        -1.08307103e-04, -0.00000000e+00, -0.00000000e+00],\n       [ 2.95222993e-02,  2.71448258e-02,  2.44086250e-02, ...,\n         2.29255562e-04, -0.00000000e+00, -0.00000000e+00],\n       [ 3.82707958e-02,  3.12527232e-02,  2.52544181e-02, ...,\n        -2.25480573e-04,  0.00000000e+00,  0.00000000e+00]])"
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Predicting light source direction from face images with a CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"deeplearning/train/labels.csv\",header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "le = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 518,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "outputs": [],
   "source": [
    "def show_face(image, camera_position):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.title(camera_position)\n",
    "    plt.imshow(image,cmap=\"gray\")\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "outputs": [],
   "source": [
    "def encode_dataframe(df):\n",
    "    df[4] = df[1].astype(str) + \",\" + df[2].astype(str) +\",\"+ df[3].astype(str)\n",
    "    df[4]  = le.fit_transform(df[4])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Custom Fashion MNIST dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, image_dir,):\n",
    "        self.labels_df = pd.read_csv(csv_file,header=None)\n",
    "        self.labels_df = encode_dataframe(self.labels_df)\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.image_dir,\n",
    "                                self.labels_df.iloc[idx, 0])\n",
    "        image = io.imread(img_name, as_gray=True)\n",
    "        image = transform.resize(image,(160,160))\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        image = convert_tensor(image)\n",
    "        image.unsqueeze(0)\n",
    "        camera_position = torch.tensor(self.labels_df.iloc[idx, 4])\n",
    "        camera_position =  camera_position.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return image.float(), camera_position,img_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"deeplearning/train/labels.csv\",header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2    3   4\n249   0250.jpg -0.087156 -0.996195  0.0   0\n937   0938.jpg -0.087156 -0.996195  0.0   0\n1703  1704.jpg -0.087156 -0.996195  0.0   0\n313   0314.jpg -0.087156 -0.996195  0.0   0\n1127  1128.jpg -0.087156 -0.996195  0.0   0\n...        ...       ...       ...  ...  ..\n1518  1519.jpg  1.000000  0.000000  0.0  63\n1070  1071.jpg  1.000000  0.000000  0.0  63\n1134  1135.jpg  1.000000  0.000000  0.0  63\n1582  1583.jpg  1.000000  0.000000  0.0  63\n0     0001.jpg  1.000000  0.000000  0.0  63\n\n[1838 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249</th>\n      <td>0250.jpg</td>\n      <td>-0.087156</td>\n      <td>-0.996195</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>937</th>\n      <td>0938.jpg</td>\n      <td>-0.087156</td>\n      <td>-0.996195</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1703</th>\n      <td>1704.jpg</td>\n      <td>-0.087156</td>\n      <td>-0.996195</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>313</th>\n      <td>0314.jpg</td>\n      <td>-0.087156</td>\n      <td>-0.996195</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1127</th>\n      <td>1128.jpg</td>\n      <td>-0.087156</td>\n      <td>-0.996195</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1518</th>\n      <td>1519.jpg</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>1071.jpg</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>1134</th>\n      <td>1135.jpg</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>1582</th>\n      <td>1583.jpg</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0001.jpg</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>63</td>\n    </tr>\n  </tbody>\n</table>\n<p>1838 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_dataframe(df2).sort_values(by=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [],
   "source": [
    "batch_size=32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [],
   "source": [
    "train_dataset = FaceDataset(csv_file='deeplearning/train/labels.csv',\n",
    "                            image_dir='deeplearning/train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "outputs": [],
   "source": [
    "test_dataset = FaceDataset(csv_file='deeplearning/validate/labels.csv',\n",
    "                           image_dir='deeplearning/validate')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [],
   "source": [
    "train_split, validation_split = random_split(train_dataset,[1470,368])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_split, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(validation_split, batch_size=batch_size,\n",
    "                                   shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "outputs": [],
   "source": [
    "def label_decode(label):\n",
    "    temp = le.inverse_transform(label)\n",
    "    temp = temp.reshape(1,-1).flatten()\n",
    "    temp = temp[0].split(\",\")\n",
    "\n",
    "    return torch.tensor(np.array(temp).astype(float))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "outputs": [],
   "source": [
    "def angular_error(output,label):\n",
    "    total_error  = 0\n",
    "    for output_val,predic_val in zip(map(label_decode,[ [x] for x in output]),map(label_decode,[[x] for x in label])):\n",
    "        total_error += torch.acos(torch.min(torch.tensor(1),torch.max(torch.tensor(-1),torch.dot(output_val,predic_val))))\n",
    "    return torch.rad2deg(total_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convlayers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # B x 6 x 80 x 80 after this maxpool\n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=4), # B x 12 x20 x 20,\n",
    "            nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=0),# B x 48 x 18x 18,\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_features=24*18*18,out_features=120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120,out_features=84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=84,out_features=64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # Input x has dimensions B x 1 x 28 x 28, B is batch size\n",
    "        x = self.convlayers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.MLP(x)\n",
    "        # Output has dimensions B x 10\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "learning_rate =0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_69068/3740275559.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# Initialise some variables for computing and tracking stats\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0miterations_per_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mceil\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mtraining_losses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mtraining_accuracies\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up the optimiser\n",
    "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "images,labels, _= next(iter(train_dataloader))\n",
    "# Initialise some variables for computing and tracking stats\n",
    "iterations_per_epoch = math.ceil(len(train_dataset)/batch_size)\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # One epoch on the training set\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    angle_error = 0\n",
    "    for i, (inputs, labels,_) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_func(output,labels)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pred_y = torch.argmax(output, 1)\n",
    "        angle_error += angular_error(pred_y.to(\"cpu\"),labels.to(\"cpu\"))\n",
    "        correct += (pred_y == labels).sum()\n",
    "        total += float(labels.size(0))\n",
    "        total_loss += loss*images.shape[0]\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, iterations_per_epoch, loss.item()))\n",
    "    total_loss /= len(train_dataset)\n",
    "    training_losses.append(total_loss.item())\n",
    "    training_accuracies.append(correct/total)\n",
    "    print('Train accuracy over epoch {}: {:.4f}'.format(epoch+1,training_accuracies[-1]))\n",
    "    print(f\"Mean Angular Error ={angle_error/len(train_dataset)}\")\n",
    "\n",
    "    # One epoch on the validation set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels,_ in validation_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_func(output,labels)\n",
    "            pred_y = torch.argmax(output, 1)\n",
    "            correct += (pred_y == labels).sum()\n",
    "            total += float(labels.size(0))\n",
    "            total_loss += loss*images.shape[0]\n",
    "        validation_accuracy = correct/total\n",
    "    total_loss /= len(validation_split)\n",
    "    validation_losses.append(total_loss.item())\n",
    "    # Switch back to training mode\n",
    "    model.train()\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    print('Test accuracy at epoch {}: {:.4f}'.format(epoch+1,validation_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.title(\"Training curve\")\n",
    "plt.plot(range(len(training_losses)),training_losses,'r')\n",
    "plt.plot(range(len(validation_losses)),validation_losses,'g')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "angle_errors = []\n",
    "for index, (images, labels,image_names) in enumerate(test_dataloader):\n",
    "    images,labels = images.to(device), labels.to(device)\n",
    "    output = model(images)\n",
    "    pred_y = torch.argmax(output, 1)\n",
    "    figure = plt.figure(figsize=(30, 15))\n",
    "    for i in range(32):\n",
    "        figure.add_subplot(8, 4, i+1)\n",
    "        error = torch.acos(torch.min(torch.tensor(1),torch.max(torch.tensor(-1),torch.dot(label_decode([pred_y[i].item()]),label_decode([labels[i].item()])))))\n",
    "        angle_errors.append(error)\n",
    "        plt.title(f\"{image_names[i]} \\n Y': {le.inverse_transform([pred_y[i].item()])} \\n Y: {le.inverse_transform([labels[i].item()])} \\n Error = {error}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(images[i,:].squeeze().cpu(), cmap=\"gray\")\n",
    "    figure.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "error = sum(angle_errors)/len(angle_errors)\n",
    "error = np.asarray(error)\n",
    "print(f\" Mean Angle Error is :{np.round(error,2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Generating face images with controllable lighting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "outputs": [],
   "source": [
    "train_dataset = FaceDataset(csv_file='deeplearning/train/labels.csv',\n",
    "                            image_dir='deeplearning/train')\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "nz = 100\n",
    "ngf = 128\n",
    "ndf = 128\n",
    "num_epochs = 7\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_69068/19224574.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m \u001B[0mnetG\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m \u001B[0mnetG\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnetG\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_69068/19224574.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      4\u001B[0m         self.main = nn.Sequential(\n\u001B[0;32m      5\u001B[0m             \u001B[1;31m# input is Z, going into a convolution\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m             \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConvTranspose2d\u001B[0m\u001B[1;33m(\u001B[0m \u001B[0mnz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mngf\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m             \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBatchNorm2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mngf\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m8\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m             \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mReLU\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nz' is not defined"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "            # (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netG = Generator()\n",
    "netG = netG.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_69068/3301265834.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\Sam\\AppData\\Local\\Temp/ipykernel_69068/3301265834.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    lass Discriminator(nn.Module):\u001B[0m\n\u001B[1;37m         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "def __init__(self, ):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "        # input is (nc) x 64 x 64\n",
    "        nn.Conv2d(1, ndf, 4, 2, 1),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf) x 32 x 32\n",
    "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "        nn.BatchNorm2d(ndf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*2) x 16 x 16\n",
    "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "        nn.BatchNorm2d(ndf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*4) x 8 x 8\n",
    "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n",
    "        nn.BatchNorm2d(ndf * 8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*8) x 4 x 4\n",
    "        nn.Conv2d(ndf * 8, 1, 4, 1, 0),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "def forward(self, input):\n",
    "    return self.main(input)\n",
    "\n",
    "netD = Discriminator()\n",
    "netD = netD.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(8, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(utils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(utils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "nsamples = 7\n",
    "z1 = torch.randn(1, nz, 1, 1, device=device)\n",
    "z2 = torch.randn(1, nz, 1, 1, device=device)\n",
    "z = torch.zeros(nsamples,nz,1,1,device=device)\n",
    "for i in range(nsamples):\n",
    "    w1 = i/(nsamples-1)\n",
    "    w2 = 1-w1\n",
    "    z[i,:,:,:] = w1*z1 + w2*z2\n",
    "images = netG(z)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 20))\n",
    "for i in range(nsamples):\n",
    "    figure.add_subplot(1, nsamples, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    fake_image = np.transpose(images[i].detach().cpu(),(1,2,0))\n",
    "    plt.imshow(fake_image, cmap=\"gray\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}